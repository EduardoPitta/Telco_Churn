{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto: Análise e Predição de Churn de acordo com o perfil de clientes\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"telco.jpeg\" >\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "**Churn é um grande problema para empresas de telecomunicação**. Com concorrência acirrada, adoção de novas (e custosas) tecnologias e altos custos de manutenção manter sua base de clientes é primordial para se manter relevante no mercado. Saber quais clientes podem deixar de assinar seus serviços pode evitar um churn em massa que pode desestabilizar suas operações.\n",
    "\n",
    "\n",
    "Este projeto usa uma base de dados disponibilizada no [Kaggle](https://www.kaggle.com/blastchar/telco-customer-churn?select=WA_Fn-UseC_-Telco-Customer-Churn.csv) e visa analizar os clientes de uma empresa fictícia de telecom usando as várias técnicas aprendidas no curso de MBA de Data Science and Analytics do PECEGE - USP. \n",
    "\n",
    "As técnicas que pretendo usar aqui são:\n",
    "\n",
    "- Análise Exploratória de Dados\n",
    "- Arvore de Decisão\n",
    "- Floresta de Decisão\n",
    "- Logistica Binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para começar, vamos estudar nossos dados vendo suas características.\n",
    "\n",
    "Temos uma base de dados com 7043 linhas e 21 colunas distintas:\n",
    "- Customer ID: Identificação de clientes, todas linhas únicas.\n",
    "- Gender: Genero biológico da pessoa (Homem ou Mulher)\n",
    "- SeniorCitizen: Se a pessoa é idosa (0 para não 1 para sim)\n",
    "- Partner: Se a pessoa tem um parceiro/a (0 para não 1 para sim)\n",
    "- Dependents: Se a pessoa tem dependentes (Yes or No)\n",
    "- Tenure: A quantos meses o cliente é assinante\n",
    "- PhoneService: Se a pessoa tem serviço de telefone ou não (Yes or No)\n",
    "- Multiple Lines: Se a pessoa tem mais de uma linha (Yes, No or No Phone Service)\n",
    "- Internet Service: Se a pessoa assina internet (DSL, Fiber Optic, No)\n",
    "- OnlineSecurity: se ela assina o serviço de segurança online (Yes, No, No internet service)\n",
    "- OnlineBackup: Se o cliente tem serviço de backup online (Yes, No, No internet service)\n",
    "- DeviceProtection: Se o cliente tem proteção para seu dispositivo (Yes, No, No internet service)\n",
    "- TechSupport: Se o cliente assina o suporte técnico (Yes, No, No internet service)\n",
    "- StreamingTV: Se o cliente assina o serviço de streaming de TV (Yes, No, No internet service)\n",
    "- StreamingMovies: Se o cliente assina o serviço de streaming de filmes (Yes, No, No internet service)\n",
    "- Contract: Qual o tipo de contrato o cliente tem (mês a mês, um ano, dois anos)\n",
    "- Paperless Billing: Se o cliente optou por faturas eletrônicas (Yes or No)\n",
    "- PaymentMethod: Método de pagamento ('Electronic check', 'Mailed check', 'Bank transfer (automatic)', 'Credit card (automatic)')\n",
    "- MonthCharges: Custos mensais do cliente\n",
    "- TotalCharges: Custos totais do cliente\n",
    "- Churn: Se o cliente saiu da empresa ou não, nossa variável alvo\n",
    "\n",
    "Vamos dar uma olhada no resumo desses dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import collections, numpy\n",
    "\n",
    "df = pd.read_csv('churn.csv')\n",
    "\n",
    "print(df.head())\n",
    "df.describe()\n",
    "df['Contract'].unique()\n",
    "#df['PaymentMethod'].unique()\n",
    "\n",
    "# A coluna TotalCharges está como string, vou substituir valores nulos ou vazios por 0 e converter para float.\n",
    "df['TotalCharges'].replace([' ','',' '],0,inplace=True)\n",
    "df['TotalCharges'] = df['TotalCharges'].astype({'TotalCharges': float})\n",
    "\n",
    "\n",
    "#print('\\nPorcentagem de Homens: %', (df['gender'].value_counts()[0]*100)/df['gender'].count())\n",
    "#print('\\nPorcentagem de Idosos: %', (df['SeniorCitizen'].value_counts()[1]*100)/df['SeniorCitizen'].count())\n",
    "#print('\\nPorcentagem de Pessoas com Parceiros: %', (df['Partner'].value_counts()[1]*100)/df['Partner'].count())\n",
    "#print('\\nPorcentagem de Pessoas com Dependentes: %', (df['Dependents'].value_counts()[1]*100)/df['Dependents'].count())\n",
    "#print('\\nPorcentagem de Churn: %', (df['Churn'].value_counts()[1]*100)/df['Churn'].count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não temos linhas sem valores, então não precisamos remediar isso na nossa base de dados.\n",
    "Vamos ver a distribuição de valores de nossas variáveis por meio de histogramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig , ax = plt.subplots(nrows = 6, ncols = 3)\n",
    "variaveis = ['gender','SeniorCitizen','Partner','Dependents','PhoneService','MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV',\n",
    "'StreamingMovies','Contract','PaperlessBilling','PaymentMethod']\n",
    "\n",
    "i=0\n",
    "j=0\n",
    "\n",
    "for variavel in variaveis:\n",
    "    sns.countplot(data = df, x = variavel, ax=ax[i][j])\n",
    "    ax[i][j].set_title(variavel,fontdict= {'fontsize': 24})\n",
    "    j = j + 1\n",
    "    if j > 2:\n",
    "        j = 0\n",
    "        i = i + 1\n",
    "\n",
    "\n",
    "sns.countplot(data = df, x = 'PaymentMethod', ax=ax[5][0])\n",
    "labels = df['PaymentMethod'].unique()\n",
    "ax[5][0].set_title('Método de Pagamento',fontdict= {'fontsize': 24})\n",
    "ax[5][0].set_xticklabels(labels,rotation=30, ha= 'right')\n",
    "sns.histplot(data = df, x = 'MonthlyCharges', ax=ax[5][1])\n",
    "ax[5][1].set_title('Faturas Mensais',fontdict= {'fontsize': 24})\n",
    "sns.countplot(data = df, x = 'Churn', ax=ax[5][2])\n",
    "ax[5][2].set_title('Churn',fontdict= {'fontsize': 24})\n",
    "\n",
    "sns.set(rc={'figure.figsize':(20,30)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = df.sort_values(['tenure']).reset_index(drop=True)\n",
    "\n",
    "fig , ax = plt.subplots(nrows = 2, ncols = 1)\n",
    "sns.countplot(data = df, x = 'tenure', ax=ax[0])\n",
    "ax[0].set_title('Meses Cliente',fontdict= {'fontsize': 28})\n",
    "sns.histplot(data = df, x = 'TotalCharges', ax=ax[1], bins = 20)\n",
    "ax[1].set_title('Faturamento Total',fontdict= {'fontsize': 28})\n",
    "\n",
    "labels = df['tenure'].unique()\n",
    "ax[0].set_xticklabels(labels,rotation = 90, ha= 'right')\n",
    "\n",
    "\n",
    "sns.set(rc={'figure.figsize':(50,35)},font_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "produtos = df.copy(deep=True)\n",
    "produtos = produtos.replace(['DSL','Fiber optic'],'Yes')\n",
    "\n",
    "def internet_e_telefone(row):\n",
    "    if row['InternetService'] == 'Yes' and row['PhoneService'] == 'Yes':\n",
    "        val = 'Both'\n",
    "    if row['InternetService'] == 'No' and row['PhoneService'] == 'Yes':\n",
    "        val = 'Phone'\n",
    "    if row['InternetService'] == 'Yes' and row['PhoneService'] == 'No':\n",
    "        val = 'Internet'\n",
    "    return val\n",
    "\n",
    "def servico_extra(row):\n",
    "    val = 'No'\n",
    "    if row['OnlineSecurity'] == 'Yes' or row['OnlineBackup'] == 'Yes' or row['DeviceProtection'] == 'Yes' or row['TechSupport'] == 'Yes' or row['StreamingTV'] == 'Yes' or row['StreamingMovies'] == 'Yes':\n",
    "        val = 'Yes'\n",
    "    return val\n",
    "\n",
    "\n",
    "produtos['TelefoneInternet'] = produtos.apply(internet_e_telefone,axis=1)\n",
    "produtos['ContrataServiço'] = produtos.apply(servico_extra,axis=1)\n",
    "\n",
    "\n",
    "fig , ax = plt.subplots(nrows = 4, ncols = 3)\n",
    "font_size = 15\n",
    "\n",
    "i=0\n",
    "j=0\n",
    "\n",
    "variaveis = ['PhoneService','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies','TelefoneInternet','ContrataServiço']\n",
    "\n",
    "\n",
    "for variavel in variaveis:\n",
    "    sns.countplot(data = produtos, x = variavel, ax=ax[i][j])\n",
    "    ax[i][j].set_title(variavel,fontdict= {'fontsize': font_size})\n",
    "    j = j + 1\n",
    "    if j > 2:\n",
    "        j = 0\n",
    "        i = i + 1\n",
    "\n",
    "\n",
    "\n",
    "sns.set(rc={'figure.figsize':(20,15)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos muitos dados interessantes em nossa base! É possível tirar vários insights!\n",
    "\n",
    "\n",
    "Para melhor organizar as idéias, vamos usar bullet points:\n",
    "- Os assinantes do serviço são homens, porém com uma pequena margem de vantagem (50,48% de homens);\n",
    "- Uma pequena parcela dos assinantes são idosos (16%);\n",
    "- Quase metade dos assinantes tem parceiros (48%);\n",
    "- Os assinantes com dependentes fazem 30% da base de clientes.\n",
    "- O serviço de telefonia é o produto mais popular;\n",
    "- A maioria das pessoas assina a internet e o serviço de telefonia simultâneamente;\n",
    "- Dos que contratam o serviço de Internet, a grande maioria contrata serviços adicionais como Streaming, Backup Online, etc...\n",
    "- O serviço mais popular é o de Streaming de TV e Filmes e o menos popular é o de Segurança Online;\n",
    "- Pelo gráfico de __tenure__ podemos ver que a maioria dos clientes da empresa são recém chegados ou já assinam seus produtos a mais de 71 meses;\n",
    "- Tivemos um Churn de **26%** neste mês, ou seja, 1/4 de nossa base de clientes foi embora em um mês!!\n",
    "\n",
    "\n",
    "Nessa próxima parte vamos analisar qual a porcentagem de churn para cada grupo de clientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para plotar gráficos Churn vs Coluna\n",
    "def plot_churn(df,column):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,20))\n",
    "    tmp1 = pd.crosstab(index = df[column], columns = df['Churn'], values = df['Churn'],\n",
    "                               aggfunc = len,\n",
    "                               normalize = 'index').mul(100)\n",
    "\n",
    "    tmp2 = df[column].value_counts(normalize=True).mul(100)\n",
    "    tmp1.plot.bar(stacked=True,figsize=(18,5),ax=ax[0], ylabel='%')\n",
    "    tmp2.plot.bar(stacked=True,figsize=(18,5),ax=ax[1], ylabel='%')\n",
    "    ax[1].set_title('{} Percentage'.format(column))\n",
    "    ax[0].set_title('Churn Vs {}'.format(column))\n",
    "    \n",
    "\n",
    "\n",
    "#Churn por serviços\n",
    "plot_churn(produtos,'PhoneService')\n",
    "#Churn por Tipo de Serviço de Internet\n",
    "plot_churn(df,'InternetService')\n",
    "#Churn por serviço de internet\n",
    "plot_churn(produtos,'InternetService')\n",
    "#Churn por Tipo de Serviço de Internet\n",
    "plot_churn(produtos,'TelefoneInternet')\n",
    "#Churn por cliente que contrata servios de internet extras\n",
    "plot_churn(produtos,'ContrataServiço')\n",
    "#Churn por genêro\n",
    "plot_churn(produtos,'gender')\n",
    "#Churn por faixa de idade\n",
    "plot_churn(produtos,'SeniorCitizen')\n",
    "#Churn por situação marital\n",
    "plot_churn(produtos,'Partner')\n",
    "#Churn por dependentes\n",
    "plot_churn(produtos,'Dependents')\n",
    "#Churn por contratante de segurança online\n",
    "plot_churn(produtos,'OnlineSecurity')\n",
    "#Churn por contratante de backup online\n",
    "plot_churn(produtos,'OnlineBackup')\n",
    "#Churn por contratante de backup online\n",
    "plot_churn(produtos,'StreamingTV')\n",
    "#Churn por contratante de backup online\n",
    "plot_churn(produtos,'StreamingMovies')\n",
    "#Churn por contratante de backup online\n",
    "plot_churn(produtos,'TechSupport')\n",
    "#Churn por contratante de backup online\n",
    "plot_churn(produtos,'DeviceProtection')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novamente é possível de observar tendências bem interessantes no dados:\n",
    "\n",
    "- Pessoas que contratam serviços de Internet extra tem uma taxa bem menor de Churn;\n",
    "- Entre os assinantes de serviços de internet, os que mais deram churn foram os assinantes de Fibra Ótica;\n",
    "- Clientes que só tem o serviço de telefone tem churn menor que clientes que tem ambos Telefone e Internet;\n",
    "- O Churn é ligeiramente menor para assinantes de streamming TV e Filmes;\n",
    "- O Churn é bem menor para clientes que assinam Suporte Tecnico e Segurança Online;\n",
    "- Pessoas sem dependentes tem churn maior que as com dependentes.\n",
    "\n",
    "Vamos agora começar a parte de implementação de modelos preditivos com uma arvore de decisão simples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árvores de Decisão\n",
    "\n",
    "\n",
    "O algoritmo de Árvore de decisão é um algortimo preditivo supervisionado, não parâmetrico, de Machine Learning, com estrutura hierarquica em forma de ávore com nó raiz, nós internos, galhos e folhas. O algoritmo visa criar uma sequência de segmentações binárias para alcançar a homogeniedade da variável resposta.\n",
    "\n",
    "Para medir essa homogeniedade dentro das folhas é normalmente usado o indice Gini ou Entropia de Shannon, onde a impureza máxima é alcançada na distribuição uniforme e impureza mínima na concentração total.\n",
    "\n",
    "O Algoritmo funciona deste modo:\n",
    "\n",
    "1. Para cada variável, buscar a melhor regra binária\n",
    "2. Escolher aplicar melhor segmentação dentre todas as variáveis\n",
    "3. Recursivamente, para cada folha, repetir os passos 1 e 2 até que\n",
    "uma regra de parada seja atingida\n",
    "\n",
    "Após a criação de uma ávore podemos mudar os hiperparâmetros, que são parâmetros que controlam o algoritmo como:\n",
    "1. Número mínimo de observações por folha\n",
    "2. Profundidade máxima\n",
    "3. CP – Custo de complexidade\n",
    "\n",
    "E temos o nível de complexidade da árvore, que pode ser baixo, médio e alto.\n",
    "\n",
    "Para cada variável vai ser criado um nó na árvore, com uma decisão binária, sendo que os primeiros nós são escolhidos de acordo com a variável que melhor consegue uma resposta homogenea daquele ponto.\n",
    "\n",
    "As variáveis que vamos usar aqui para descobrir quem vai desistir dos serviços da empresa são:\n",
    "\n",
    "- Gender: Genero biológico da pessoa (Homem ou Mulher)\n",
    "- SeniorCitizen: Se a pessoa é idosa (0 para não 1 para sim)\n",
    "- Partner: Se a pessoa tem um parceiro/a (0 para não 1 para sim)\n",
    "- Dependents: Se a pessoa tem dependentes (Yes or No)\n",
    "- PhoneService: Se a pessoa tem serviço de telefone ou não (Yes or No)\n",
    "- Multiple Lines: Se a pessoa tem mais de uma linha (Yes, No or No Phone Service)\n",
    "- Internet Service: Se a pessoa assina internet (DSL, Fiber Optic, No)\n",
    "- OnlineSecurity: se ela assina o serviço de segurança online (Yes, No, No internet service)\n",
    "- OnlineBackup: Se o cliente tem serviço de backup online (Yes, No, No internet service)\n",
    "- DeviceProtection: Se o cliente tem proteção para seu dispositivo (Yes, No, No internet service)\n",
    "- TechSupport: Se o cliente assina o suporte técnico (Yes, No, No internet service)\n",
    "- StreamingTV: Se o cliente assina o serviço de streaming de TV (Yes, No, No internet service)\n",
    "- StreamingMovies: Se o cliente assina o serviço de streaming de filmes (Yes, No, No internet service)\n",
    "- Contract: Qual o tipo de contrato o cliente tem (mês a mês, um ano, dois anos)\n",
    "- Paperless Billing: Se o cliente optou por faturas eletrônicas (Yes or No)\n",
    "- PaymentMethod: Método de pagamento ('Electronic check', 'Mailed check', 'Bank transfer (automatic)', 'Credit card (automatic)')\n",
    "\n",
    "E nossa variável alvo é:\n",
    "- Churn: Se o cliente saiu da empresa ou não (Yes or No)\n",
    "\n",
    "Primeiramente devemos separar nosso banco de dados entre treino e teste.\n",
    "Esse passo é importante para sabermos se, de fato, nosso modelo vai funcionar em uma ocasião real, e nesse mesmo bloco vamos construir nossa arvore de decisão \"padrão\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#Como esse algoritmo só funciona com float, vamos ter que fazer labels numericas.\n",
    "\n",
    "\n",
    "X = df.drop(['Churn','customerID'], axis=1)\n",
    "y = df['Churn']\n",
    "print(y)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Vamos trocar labels em strings por labels numéricas no train seguindo:\n",
    "#Passo 1 - Ident. colunas categoricas\n",
    "#Passo 2- Aplicar o label encoder somente nas categoricas\n",
    "#Guardar esse objeto para usar no teste\n",
    "\n",
    "colunas_categoricas =[]\n",
    "\n",
    "for column in X_train:\n",
    "    try:\n",
    "        X_train[column].iloc[0].isnumeric() == False\n",
    "\n",
    "    except AttributeError:\n",
    "        print (type(X_train[column][0]))\n",
    "\n",
    "    else:\n",
    "        colunas_categoricas.append(column)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for coluna in colunas_categoricas:\n",
    "    le.fit(X_train[coluna])\n",
    "    X_train[coluna] = le.transform(X_train[coluna])\n",
    "    X_test[coluna] = le.transform(X_test[coluna])\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "\n",
    "#Vamos fazer nossa árvore de decisão setando todos os parâmetros como default\n",
    "\n",
    "\n",
    "avore_de_decisao = DecisionTreeClassifier(random_state=0,\n",
    "                                          criterion=\"gini\",\n",
    "                                          splitter=\"best\",\n",
    "                                          max_depth=None,\n",
    "                                          min_samples_split=2,\n",
    "                                          min_samples_leaf=1,\n",
    "                                          max_leaf_nodes=None,\n",
    "                                          min_impurity_decrease=0,\n",
    "                                          cpp_alpha=0)\n",
    "\n",
    "avore_de_decisao.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = avore_de_decisao.predict(X_test)\n",
    "\n",
    "training_score = cross_val_score(avore_de_decisao, X_train, y_train, cv=5)\n",
    "print(\"O método de arvore de decisão teve um score de treino de: \", round(training_score.mean(), 2) * 100, \"% de acurácia e um F1-Score de:\", f1_score(y_test, y_predicted))\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(25,20))\n",
    "# _ = tree.plot_tree(arvore_de_decisao,   \n",
    "#                   class_names='Churn',\n",
    "#                    filled=True)\n",
    "\n",
    "# fpr, tpr, _ = roc_curve(y_test, y_predicted)\n",
    "\n",
    "# plt.clf()\n",
    "# plt.plot(fpr, tpr)\n",
    "# plt.xlabel('FPR')\n",
    "# plt.ylabel('TPR')\n",
    "# plt.title('ROC curve')\n",
    "# plt.show()\n",
    "\n",
    "# path = avore_de_decisao.cost_complexity_pruning_path(X_train,y_train)\n",
    "# alphas=path['ccp_alphas']\n",
    "\n",
    "# alphas\n",
    "\n",
    "# confusion_matrix = pd.crosstab(y_test, y_predicted, rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "# sns.heatmap(confusion_matrix, annot=True)\n",
    "# plt.show()\n",
    "\n",
    "#https://towardsdatascience.com/demystifying-roc-curves-df809474529a\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checagem de resultados e métricas de performance\n",
    "\n",
    "Para seguirmos com o ajuste de hiperparâmetros de nosso modelo, primeiro precisamos nos perguntar: Qual é a melhor métrica para nosso problema?\n",
    "Precisamos saber se nosso modelo está bom ou ruim e levar em conta a estrutura dele e o problema que queremos resolver. Algumas métricas que podeusar usar são:\n",
    "\n",
    "* Precisão:\n",
    "    A precisão de um modelo é a proporção de quantas observações ele consegue \"adivinhar\" corretamente, normalmente medida em porcentagem. Se ele acertar 8 observações de 10, ele terá 80% de precisão. Porém aqui podemos ter um problema, suponhamos que em nossos dados tenhamos 90% de observações \"1\" e 10% de observações \"2\". Isso quer dizer que um modelo que somente retorne \"1\" tem 90% de precisão! Isso não diz nada sobre a qualidade do modelo e precisariamos de outra métrica para avaliar nosso modelo.\n",
    "\n",
    "* F1-Score:\n",
    "    F-1 Score é uma medida de precisão em um dataset, e é usado para avaliar sistemas de classificação binária, como é o caso de nossa árvore de decisão. Ele é composto pela média harmonica entre recall (ou sensibilidade) e precisão. A formula pode ser escrita como 2*((precisão*recall)/(precisão+recall)) ou tp/(tp+(fp+fn)/2), onde tp é \"verdadeiro positivo\", fp é \"falso positivo\" e fn é \"falso negativo\".\n",
    "\n",
    "* Fbeta-Score:\n",
    "    É uma adaptação do F1-Score onde colocamos mais importância para a precisão ou sensibilidade de um modelo. A formula é ( (1+b^2)*tp) / ((1+b^2)*tp + b^2*fn + fp) onde quanto maior o b maior importância vai ter o recall.\n",
    "\n",
    "Agora, uma breve análise de nosso problema de negócios: Queremos evitar que clientes com o perfil de serem churn de fato virem churn, então se nosso modelo ser uma ótima precisão, mas uma sensibilidade baixa, não estamos resolvendo nosso problema pois estariamos classicando muitos clientes em risco de churn como falsos negativos e consequentemente não conseguiriamos direcionar nossos esforços de marketing para as pessoas corretas.\n",
    "\n",
    "Vendo desta forma, julguei como a melhor métrica para nosso projetinho o Fbeta score, dando prioridade para uma maior sensibilidade de nosso modelo!\n",
    "\n",
    "Agora podemos pensar em como ajustar os hiperparâmetros de nossa árvore para resolvermos nosso problema de negócio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajustando os Hiperparâmetros\n",
    "\n",
    "Agoravamos ajustar os hiperparâmetros da árvore para conseguirmos generalizar nosso modelo, evitando overfit, e conseguir resolver nosso problema.\n",
    "\n",
    "O primeiro hiperparâmetro que vamos estudar é o max_depth, que indica o quão \"profunda\" uma árvore de decisão pode ir. Quanto mais profunda mais informação dos dados ela irá capturar, mas daí corremos o risco de fazer um modelo que vai muito bem nos dados de treino, mas ruim nos dados de validação ou teste. Isso se chama overfitting.\n",
    "\n",
    "O próximo é o hiperparâmetro min_samples_split, esse hiperparâmetro especifica o número minimo de amostras necessárias para dividir um nó interno. Se por exemplo, chegnado a um nó, só termos 500 observações e nosso parâmetro estiver setado em 700, o split não vai ocorrer.\n",
    "\n",
    "Outro hiperparâmetro é o min_samples_leaf, um nó folha, ou só folha, é um nó tem filhos, ou a saída de nossa árvore. Esse parâmetro é o número mínimo de amostras necessárias para ser uma folha.\n",
    "\n",
    "E o último é o max_features, que representa o número de features, ou variáveis, levadas em conta quando procuramos pelo melhor \"split\". ele leva como entrada um inteiro para denotar o numero máximo de variáveis para para \"split\" ou porcentagem de variáveis para considerar quando se faz um split.\n",
    "\n",
    "O critério que vamos usar é o índice Gini para verificar a homogeniedade de nossos nós.\n",
    "\n",
    "Vamos mudar cada um dos hiperparâmetros gradualmente, e colocar em um gráfico qual foi a mudança no nosso Fb-Score para cada um deles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "#Vamos variar max_depth de 3 até 50, para vermos como nosso overfitting e Kb-Score fica a cada nível que a arore cresce!    \n",
    "#Aqui vamos adotar um b = 1.5.\n",
    "\n",
    "accuracy_train,accuracy_test,f1_train,f1_test=[],[],[],[]\n",
    "\n",
    "for i in range(3,31):\n",
    "    arvore_de_decisao = DecisionTreeClassifier(random_state=0,\n",
    "                                          criterion=\"gini\",\n",
    "                                          splitter=\"best\",\n",
    "                                          max_depth=i,\n",
    "                                          min_samples_split=2,\n",
    "                                          min_samples_leaf=1,\n",
    "                                          max_leaf_nodes=None,\n",
    "                                          min_impurity_decrease=0\n",
    "                                          )\n",
    "    arvore_de_decisao.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "    if i < 6:\n",
    "        fig = plt.figure(figsize=(25,20))\n",
    "        _ = tree.plot_tree(arvore_de_decisao,   \n",
    "                        class_names='Churn',\n",
    "                        filled=True)\n",
    "        plt.show()\n",
    "\n",
    "    y_train_pred=arvore_de_decisao.predict(X_train)\n",
    "    y_test_pred=arvore_de_decisao.predict(X_test)\n",
    "    accuracy_train.append(accuracy_score(y_train,y_train_pred))\n",
    "    accuracy_test.append(accuracy_score(y_test,y_test_pred))\n",
    "\n",
    "    f1_train.append(f1_score(y_train,y_train_pred))\n",
    "    f1_test.append(f1_score(y_test,y_test_pred))\n",
    "\n",
    "sns.set()\n",
    "plt.figure(figsize=(14,7))\n",
    "sns.lineplot(y=accuracy_train,x=range(3,3+len(accuracy_train)), label=\"Precisão no Treino\")\n",
    "sns.lineplot(y=accuracy_test,x=range(3,3+len(accuracy_test)), label=\"Precisão no Teste\")\n",
    "plt.xticks(range(3,30,2))\n",
    "\n",
    "sns.set()\n",
    "plt.figure(figsize=(14,7))\n",
    "sns.lineplot(y=f1_train,x=range(3,3+len(f1_train)), label=\"F1 no Treino\")\n",
    "sns.lineplot(y=f1_test,x=range(3,3+len(f1_test)), label=\"F1 no Teste\")\n",
    "plt.xticks(range(3,30,2))\n",
    "plt.show()\n",
    "\n",
    "print(X_test.iloc[14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente quero comentar sobre o que achamos com as ávores plotadas aqui.\n",
    "\n",
    "Podemos ver que o fator que melhor separa as pessoas que vão fazer churn das pessoas que não fazem é o X[14], que em nosso caso é Contrato, ou de quant oe mquanto tempo uma pessoa paga sua conta (mes a mes, ano a ano). Isso é um grande insight! Será que precisamos adaptar como fazemos nossos contratos?!\n",
    "\n",
    "Agora voltando a nossa avaliação de precisão.\n",
    "Ao ver o gráfico de profundade x precisão, vemos que nosso modelo consegue um pico no f1 score quando usamos uma árvore de profundidade 5!\n",
    "Vamos agora fixar depth em 5 e variar min_samples_split entre 2 e 20 para observar o comportamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#Vamos variar max_depth de 3 até 50, para vermos como nosso overfitting e Kb-Score fica a cada nível que a arore cresce!    \n",
    "#Aqui vamos adotar um b = 1.5.\n",
    "\n",
    "accuracy_train,accuracy_test,f1_train,f1_test=[],[],[],[]\n",
    "\n",
    "for i in range(2,4001,200):\n",
    "    arvore_de_decisao = DecisionTreeClassifier(random_state=0,\n",
    "                                          criterion=\"gini\",\n",
    "                                          splitter=\"best\",\n",
    "                                          max_depth=5,\n",
    "                                          min_samples_split=i,\n",
    "                                          min_samples_leaf=1,\n",
    "                                          max_leaf_nodes=None,\n",
    "                                          min_impurity_decrease=0\n",
    "                                          )\n",
    "\n",
    "    arvore_de_decisao.fit(X_train,y_train)\n",
    "\n",
    "    if i == 2 or i == 202 or i==802 or i==2000 or i==4000:\n",
    "        fig = plt.figure(figsize=(25,20))\n",
    "        _ = tree.plot_tree(arvore_de_decisao,   \n",
    "                        class_names='Churn',\n",
    "                        filled=True)\n",
    "        plt.show()\n",
    "\n",
    "    y_train_pred=arvore_de_decisao.predict(X_train)\n",
    "    y_test_pred=arvore_de_decisao.predict(X_test)\n",
    "    accuracy_train.append(accuracy_score(y_train,y_train_pred))\n",
    "    accuracy_test.append(accuracy_score(y_test,y_test_pred))\n",
    "\n",
    "    f1_train.append(f1_score(y_train,y_train_pred))\n",
    "    f1_test.append(f1_score(y_test,y_test_pred))\n",
    "\n",
    "    y_train_pred=arvore_de_decisao.predict(X_train)\n",
    "    y_test_pred=arvore_de_decisao.predict(X_test)\n",
    "    accuracy_train.append(accuracy_score(y_train,y_train_pred))\n",
    "    accuracy_test.append(accuracy_score(y_test,y_test_pred))\n",
    "\n",
    "    f1_train.append(f1_score(y_train,y_train_pred))\n",
    "    f1_test.append(f1_score(y_test,y_test_pred))\n",
    "\n",
    "sns.set()\n",
    "plt.figure(figsize=(14,7))\n",
    "sns.lineplot(y=accuracy_train,x=range(2,2+(len(accuracy_train)*200),200), label=\"Precisão no Treino\")\n",
    "sns.lineplot(y=accuracy_test,x=range(2,2+(len(accuracy_test)*200),200), label=\"Precisão no Teste\")\n",
    "plt.xticks(range(2,len(accuracy_train)*200,200))\n",
    "\n",
    "sns.set()\n",
    "plt.figure(figsize=(14,7))\n",
    "sns.lineplot(y=f1_train,x=range(2,2+(len(f1_train)*200),200), label=\"F1 no Treino\")\n",
    "sns.lineplot(y=f1_test,x=range(2,2+(len(f1_test)*200),200), label=\"F1 no Teste\")\n",
    "plt.xticks(range(2,len(accuracy_train)*200,200))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao que podemos observar, maior o mínimo de observações para cada split, mais simples a árvore fica, tendendo a arvore de decisão termianr mais cedo dependendo da variável e nível na árvore.\n",
    "Analisando os gráficos, podemos ver que o valor ideal para min_samples_split está entre 2 e 202. Será que usar 202 valeria a pena por diminuir a complexidade da árvore e obter os mesmos valores de precisão e f1 score?\n",
    "\n",
    "Agora, vamos analizar o comportamento do hiperparâmetro min_samples_leaf=i.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#Vamos variar max_depth de 3 até 50, para vermos como nosso overfitting e Kb-Score fica a cada nível que a arore cresce!    \n",
    "#Aqui vamos adotar um b = 1.5.\n",
    "\n",
    "accuracy_train,accuracy_test,f1_train,f1_test=[],[],[],[]\n",
    "\n",
    "for i in range(2,2001,200):\n",
    "    arvore_de_decisao = DecisionTreeClassifier(random_state=0,\n",
    "                                          criterion=\"gini\",\n",
    "                                          splitter=\"best\",\n",
    "                                          max_depth=5,\n",
    "                                          min_samples_split=200,\n",
    "                                          min_samples_leaf=i,\n",
    "                                          max_leaf_nodes=None,\n",
    "                                          min_impurity_decrease=0\n",
    "                                          )\n",
    "\n",
    "    arvore_de_decisao.fit(X_train,y_train)\n",
    "\n",
    "    if i == 2 or i == 202 or i==802 or i==2000 or i==4000:\n",
    "        fig = plt.figure(figsize=(25,20))\n",
    "        _ = tree.plot_tree(arvore_de_decisao,   \n",
    "                        class_names='Churn',\n",
    "                        filled=True)\n",
    "        plt.show()\n",
    "\n",
    "    y_train_pred=arvore_de_decisao.predict(X_train)\n",
    "    y_test_pred=arvore_de_decisao.predict(X_test)\n",
    "    accuracy_train.append(accuracy_score(y_train,y_train_pred))\n",
    "    accuracy_test.append(accuracy_score(y_test,y_test_pred))\n",
    "\n",
    "    f1_train.append(f1_score(y_train,y_train_pred))\n",
    "    f1_test.append(f1_score(y_test,y_test_pred))\n",
    "\n",
    "    y_train_pred=arvore_de_decisao.predict(X_train)\n",
    "    y_test_pred=arvore_de_decisao.predict(X_test)\n",
    "    accuracy_train.append(accuracy_score(y_train,y_train_pred))\n",
    "    accuracy_test.append(accuracy_score(y_test,y_test_pred))\n",
    "\n",
    "    f1_train.append(f1_score(y_train,y_train_pred))\n",
    "    f1_test.append(f1_score(y_test,y_test_pred))\n",
    "\n",
    "sns.set()\n",
    "plt.figure(figsize=(14,7))\n",
    "sns.lineplot(y=accuracy_train,x=range(2,2+(len(accuracy_train)*200),200), label=\"Precisão no Treino\")\n",
    "sns.lineplot(y=accuracy_test,x=range(2,2+(len(accuracy_test)*200),200), label=\"Precisão no Teste\")\n",
    "plt.xticks(range(2,len(accuracy_train)*200,200))\n",
    "\n",
    "sns.set()\n",
    "plt.figure(figsize=(14,7))\n",
    "sns.lineplot(y=f1_train,x=range(2,2+(len(f1_train)*200),200), label=\"F1 no Treino\")\n",
    "sns.lineplot(y=f1_test,x=range(2,2+(len(f1_test)*200),200), label=\"F1 no Teste\")\n",
    "plt.xticks(range(2,len(accuracy_train)*200,200))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que o hiperparâmetro min_samples_leaf tem influência direta na profundidade máxima de uma árvore.\n",
    "Isso é esperado, pois ela limita a criação de folhas que não cumpram com o número de observações minimo necessário para a criação de uma folha.\n",
    "\n",
    "Ao que parece, em nosso caso, valores entre 2 e 200 neste problema parecem ser os ideais para alcançarmos uma boa assertividade do modelo.\n",
    "Porém, quando o mesmo modelo é rodado com min_samples_node = 200 temos perda de f1 score e precisão! Isso porquê perdemos nós intermediáriso e temos perda de informação na árvore.\n",
    "\n",
    "Será que precisamos analisar o comportamento destas duas variáveis ao mesmo tempo? Depois...\n",
    "\n",
    "Agora vamos mexer no parâmetro max_leaf_nodes entre 2 e 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#Vamos variar max_depth de 3 até 50, para vermos como nosso overfitting e Kb-Score fica a cada nível que a arore cresce!    \n",
    "#Aqui vamos adotar um b = 1.5.\n",
    "\n",
    "accuracy_train,accuracy_test,f1_train,f1_test=[],[],[],[]\n",
    "\n",
    "for i in range(2,51):\n",
    "    arvore_de_decisao = DecisionTreeClassifier(random_state=0,\n",
    "                                          criterion=\"gini\",\n",
    "                                          splitter=\"best\",\n",
    "                                          max_depth=5,\n",
    "                                          min_samples_split=2,\n",
    "                                          min_samples_leaf=2,\n",
    "                                          max_leaf_nodes=i,\n",
    "                                          min_impurity_decrease=0\n",
    "                                          )\n",
    "\n",
    "    arvore_de_decisao.fit(X_train,y_train)\n",
    "\n",
    "    if i == 2 or i == 10 or i==25 or i==40 or i==50:\n",
    "        fig = plt.figure(figsize=(25,20))\n",
    "        _ = tree.plot_tree(arvore_de_decisao,   \n",
    "                        class_names='Churn',\n",
    "                        filled=True)\n",
    "        plt.show()\n",
    "\n",
    "    y_train_pred=arvore_de_decisao.predict(X_train)\n",
    "    y_test_pred=arvore_de_decisao.predict(X_test)\n",
    "    accuracy_train.append(accuracy_score(y_train,y_train_pred))\n",
    "    accuracy_test.append(accuracy_score(y_test,y_test_pred))\n",
    "\n",
    "    f1_train.append(f1_score(y_train,y_train_pred))\n",
    "    f1_test.append(f1_score(y_test,y_test_pred))\n",
    "\n",
    "    y_train_pred=arvore_de_decisao.predict(X_train)\n",
    "    y_test_pred=arvore_de_decisao.predict(X_test)\n",
    "    accuracy_train.append(accuracy_score(y_train,y_train_pred))\n",
    "    accuracy_test.append(accuracy_score(y_test,y_test_pred))\n",
    "\n",
    "    f1_train.append(f1_score(y_train,y_train_pred))\n",
    "    f1_test.append(f1_score(y_test,y_test_pred))\n",
    "\n",
    "sns.set()\n",
    "plt.figure(figsize=(14,7))\n",
    "sns.lineplot(y=accuracy_train,x=range(2,2+(len(accuracy_train)),1), label=\"Precisão no Treino\")\n",
    "sns.lineplot(y=accuracy_test,x=range(2,2+(len(accuracy_test)),1), label=\"Precisão no Teste\")\n",
    "plt.xticks(range(2,len(accuracy_train),1))\n",
    "\n",
    "sns.set()\n",
    "plt.figure(figsize=(14,7))\n",
    "sns.lineplot(y=f1_train,x=range(2,2+(len(f1_train)),1), label=\"F1 no Treino\")\n",
    "sns.lineplot(y=f1_test,x=range(2,2+(len(f1_test)),1), label=\"F1 no Teste\")\n",
    "plt.xticks(range(2,len(accuracy_train),1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui nossos gráficos, tanto de precisão quanto de f1 score, tem comportamentos interessantes.\n",
    "\n",
    "Há um pico de f1 score próximo de 0.6 quando adotamos o número máximo de folhas em 4 e 5. Após o ponto x=11 a precisão consegue superar o valor máximo anterior, mas o valor de F1 não!\n",
    "\n",
    "O que será que isso significa?\n",
    "\n",
    "O último parâmetro de estudo é min_impurity_decrease=0, que varia de 0 a 1, vamos dar uma olhada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "accuracy_train,accuracy_test,f1_train,f1_test=[],[],[],[]\n",
    "\n",
    "for i in range(0,5):\n",
    "    arvore_de_decisao = DecisionTreeClassifier(random_state=0,\n",
    "                                          criterion=\"gini\",\n",
    "                                          splitter=\"best\",\n",
    "                                          max_depth=5,\n",
    "                                          min_samples_split=2,\n",
    "                                          min_samples_leaf=2,\n",
    "                                          max_leaf_nodes=None,\n",
    "                                          min_impurity_decrease=i/100\n",
    "                                          )\n",
    "\n",
    "    arvore_de_decisao.fit(X_train,y_train)\n",
    "\n",
    "    if i == 0 or i == 1 or i==2 or i==3 or i==4:\n",
    "        fig = plt.figure(figsize=(25,20))\n",
    "        _ = tree.plot_tree(arvore_de_decisao,   \n",
    "                        class_names='Churn',\n",
    "                        filled=True)\n",
    "        plt.show()\n",
    "\n",
    "    y_train_pred=arvore_de_decisao.predict(X_train)\n",
    "    y_test_pred=arvore_de_decisao.predict(X_test)\n",
    "    accuracy_train.append(accuracy_score(y_train,y_train_pred))\n",
    "    accuracy_test.append(accuracy_score(y_test,y_test_pred))\n",
    "\n",
    "    f1_train.append(f1_score(y_train,y_train_pred))\n",
    "    f1_test.append(f1_score(y_test,y_test_pred))\n",
    "\n",
    "    y_train_pred=arvore_de_decisao.predict(X_train)\n",
    "    y_test_pred=arvore_de_decisao.predict(X_test)\n",
    "    accuracy_train.append(accuracy_score(y_train,y_train_pred))\n",
    "    accuracy_test.append(accuracy_score(y_test,y_test_pred))\n",
    "\n",
    "    f1_train.append(f1_score(y_train,y_train_pred))\n",
    "    f1_test.append(f1_score(y_test,y_test_pred))\n",
    "\n",
    "sns.set()\n",
    "plt.figure(figsize=(14,7))\n",
    "sns.lineplot(y=accuracy_train,x=range(2,2+(len(accuracy_train)),1), label=\"Precisão no Treino\")\n",
    "sns.lineplot(y=accuracy_test,x=range(2,2+(len(accuracy_test)),1), label=\"Precisão no Teste\")\n",
    "plt.xticks(range(2,len(accuracy_train),1))\n",
    "\n",
    "sns.set()\n",
    "plt.figure(figsize=(14,7))\n",
    "sns.lineplot(y=f1_train,x=range(0,(len(f1_train)),1), label=\"F1 no Treino\")\n",
    "sns.lineplot(y=f1_test,x=range(0,(len(f1_test)),1), label=\"F1 no Teste\")\n",
    "plt.xticks(range(2,len(accuracy_train),1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variando a min_impurity_decrease tivemos influência direta no número de nós e folhas criadas, assim como na profundidade da árvore, setando um minimo de mudança de impureza para a criação de um novo nó.\n",
    "\n",
    "Esse parâmetro pode ser útil para evitarmos a criação de nós que não melhoram significativamente nossos resultados e adicionam complexidade a nossa árvore.\n",
    "\n",
    "Esse parâmetro deve ser mexido com cuidado, pois usa valores decimais e é necessário atenção ao modelo criado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comentários sobre o uso de Árvores de Decisão\n",
    "\n",
    "A escrever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão Logística Binária\n",
    "\n",
    "\n",
    "A regressão logística binária é um algoritmo supervisionado, tebdo fins preditivos, sendo um modelo de regressão linear generalizado, binário ou multinomial, que tem uma distribuição Bernoulli, tras um approach diferente a escolha de modelos preditivos categóricos binários. O modelo aceita features métricas e não métricas. É um método que calcula a probabilidade de uma observação ser classificada em uma categoria.\n",
    "\n",
    "\n",
    "Antes de explicar melhor este método é importante estabelecer alguns conceitos base:\n",
    "\n",
    "1. Probabilidade\n",
    "\n",
    "Seja Y a resposta a um estimulo (sim ou não) - pode ser a preferência por algum produto, ou no nosso caso, churn ou não-churn.\n",
    "p = probabilidade da resposta ser sim (ou churn)\n",
    "1-p = probabilidade da resposta ser não (ou não-churn)\n",
    "\n",
    "Para medir essa homogeniedade dentro das folhas é normalmente usado o indice Gini ou Entropia de Shannon, onde a impureza máxima é alcançada na distribuição uniforme e impureza mínima na concentração total.\n",
    "\n",
    "2. Chance(odds)\n",
    "\n",
    "A chance da ocorrência de um evento.\n",
    "\n",
    "chance = p/1-p\n",
    "se p = 0,50; chance = 1 (1 para 1)\n",
    "se p = 0,75; chance = 3 (3 para 1)\n",
    "se p = 0,25; chance = 1/3 (1 para 3)\n",
    "\n",
    "3. Logito\n",
    "\n",
    "É o logaritmo natural da chance de ocorrência de uma resposta do tipo “sim”.\n",
    "\n",
    "E a partir dele que se define a expressão da probabilidade de ocorrência do evento em um estudo, em função das features.\n",
    "\n",
    "A partir do logito é plottada a A curva logística, ou sigmóide, que descreve a relação entre a probabilidade associada à ocorrência de determinando\n",
    "evento e um conjunto de features.\n",
    "\n",
    "Como fazemos isso? E por que esse método é considerado um modelo linear gerenalizado?\n",
    "\n",
    "Porque ela usa como base um vetor linear com as features, ou variáveis explicativas, na seguinte estrutura:\n",
    "\n",
    "\n",
    "Z = a + b1*x1 + b2*x2 + b3*x3 .... by*xy\n",
    "\n",
    "\n",
    "e a igualamos isso a\n",
    "\n",
    "\n",
    "Z = ln(p/(1-p))\n",
    "\n",
    "\n",
    "e dessa equação podemos isolar o p e assim termos:\n",
    "\n",
    "\n",
    "p = 1/(1+e^(-Z))\n",
    "\n",
    "\n",
    "Lembrando novamente que Z é onde nosso vetor com os termos da equação linear estão.\n",
    "\n",
    "ou quer dizer, para estabelecermos os fatores de nossa regressão igualamos nosso vetor com o logaritmo natural das chances de um evento, isolamos p (probabilidade de evento) para\n",
    "ficar em função de nosso vetor Z!!!\n",
    "\n",
    "E para a estimação de parâmetros usamos um processo iterativo para maximizar o acerto da probabilidade de ocorrência de um evento a sua real ocorrência, pelo método da máxima verossimilhança. E lembrando que o resultado deste método é um número entre 0 e 1 para cada observação.\n",
    "\n",
    "Com isso, vamos dar inicio a construção de nosso modelo base ou \"standard\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "O método de arvore de decisão teve um score de treino de:  0.8062455642299503 % de acurácia e um F1-Score de: 0.588235294117647\n",
      "[0.4557841449194362, 0.9743688097756015, 0.9709865239244292, 0.7980549311831453, 0.5812991243986203, 0.4572606968131091, 0.5538734922704565, 0.974651743259401, 0.5088261958089029, 0.21095629143593353, 0.982367240339991, 0.5234585053962744, 0.23132745492794593, 0.9962927472328321, 0.6160992110983279, 0.5473356270545048, 0.5402411516921939, 0.6491052561987699, 0.9698135386521041, 0.9825604992909004, 0.9318969626349906, 0.9897332483825192, 0.7226353236754458, 0.7954244280398834, 0.5930298166566017, 0.7028084778424262, 0.9842249249825705, 0.9544421865612199, 0.9950140006536592, 0.9888223576207998, 0.9637585598297064, 0.4549542718504276, 0.9849774201996186, 0.9988849222603933, 0.7492978224271363, 0.4116837648389926, 0.18673321798995657, 0.7372919535222202, 0.9880855324945231, 0.9645634527236824, 0.30588684653960585, 0.38690971658795437, 0.9041396105902266, 0.748751551695261, 0.5935818785733378, 0.7451607626386308, 0.9346492305687573, 0.8559522156321631, 0.32153404750003467, 0.4837552967283354, 0.9496160267982399, 0.5006965392178355, 0.29077086087029225, 0.528177482015197, 0.38477749289732144, 0.6307618411393349, 0.9812705002396656, 0.9863792345716962, 0.5468248293281328, 0.984711813961085, 0.3542097277933348, 0.8152790857884276, 0.7367775544445483, 0.6669924837502273, 0.3245249761176985, 0.8412762101212904, 0.5968799870578569, 0.9768138793486136, 0.9820989049730233, 0.5557223619381146, 0.8475670846099173, 0.9725912533341573, 0.989805579956609, 0.23915383585070094, 0.8652844139159424, 0.25307867128641437, 0.9787580325628893, 0.9462069687278787, 0.9949374739150608, 0.4784698838762502, 0.848520389152102, 0.277506745098858, 0.8079186307045865, 0.8841932901020225, 0.4692276309025769, 0.46202559560449463, 0.3587896065697611, 0.8854553574635657, 0.8621732503814736, 0.60787752605831, 0.406368866067509, 0.993947665898978, 0.7451297315030079, 0.98743247420205, 0.6343278052986256, 0.7057420243730534, 0.4517023024228478, 0.6580581896509898, 0.416130098369234, 0.9419782759771373, 0.5894390891385561, 0.8317319107906572, 0.9808694999216828, 0.8746628840568926, 0.8914441989658879, 0.8532712861961408, 0.896552693430648, 0.956118601636849, 0.8082305508486368, 0.9488356479149033, 0.9931845085525209, 0.6261324610707113, 0.9754529700412973, 0.9984035229889813, 0.6358863825744061, 0.6730852198984754, 0.29809983195517886, 0.28525262195427703, 0.5370390853038456, 0.7156175192840812, 0.7258821382186027, 0.5064714773236549, 0.37770882350443313, 0.9079668136395939, 0.97918579209874, 0.42372569749877376, 0.9162809567847592, 0.6982954965728683, 0.9392846929604546, 0.45027061864255635, 0.4225713269961754, 0.34440504255815074, 0.7540244849653328, 0.892611273297222, 0.9950667806163784, 0.9489686956669322, 0.9978727841980561, 0.96122402119102, 0.9872284862029324, 0.9710707482935086, 0.9432610852039427, 0.36738067543238107, 0.9347618563607774, 0.9969405420342201, 0.6378904660494283, 0.9883123146780782, 0.8682035903260414, 0.9792468740297448, 0.6250553371716283, 0.7229272573633765, 0.3435049239949096, 0.9871016632875539, 0.8291626459626537, 0.3457413185242094, 0.8885123291263194, 0.6428921655037134, 0.4027658789792916, 0.6974263207841893, 0.9639960193607772, 0.5600312372449947, 0.9384241866637836, 0.466123100191858, 0.7373682279544709, 0.3553338534103966, 0.4471131324052833, 0.987710227330669, 0.9370463943562776, 0.8253622046346615, 0.912440222173145, 0.9835381897233134, 0.9222360223397676, 0.4635155510133413, 0.8232004316296022, 0.49204883142115274, 0.7884864688437558, 0.9702950998545724, 0.9759849898136111, 0.9181106173326757, 0.9152750276332956, 0.36330825890925134, 0.48696237319624114, 0.8555153071016145, 0.8460476032015152, 0.3960643492642263, 0.2366683710227, 0.9377359887621675, 0.37600580526915595, 0.9987222735329786, 0.9389025948246172, 0.9504452749915827, 0.34219002420480193, 0.9890212160432629, 0.3993477780088577, 0.8874479698985022, 0.217847748272598, 0.9977972650994257, 0.9963738007692605, 0.9439739721297198, 0.9895810396356884, 0.3685770270646357, 0.6060356693425892, 0.5280988183954474, 0.37569474455794727, 0.42635846778820663, 0.3739522435812648, 0.44517017049099095, 0.7653583954814585, 0.28985960631875607, 0.39015410808521833, 0.9489756916172445, 0.34064270491117676, 0.6420150542312286, 0.3390554753366367, 0.9937817250719322, 0.3408972972022226, 0.8554932040462446, 0.7428893311699516, 0.6475082820963545, 0.9609098695302725, 0.5192060481450511, 0.8204701382242825, 0.9550088183661011, 0.34960367820287064, 0.5522170096408465, 0.4461104549192586, 0.9635436025682002, 0.941334684024234, 0.9858159770721929, 0.8542693135368644, 0.9956109183369315, 0.3949110466226269, 0.38725187311489806, 0.8031450550727028, 0.9764541037673664, 0.35171400163860045, 0.34263568675316247, 0.8586758512116405, 0.9789419005919439, 0.9972566100161876, 0.9980283978249912, 0.9280829643017616, 0.9973224383564737, 0.5896694397125879, 0.7168407135124544, 0.5130119445782244, 0.6951912790137491, 0.8362248190745001, 0.2296933453778739, 0.3503097451305629, 0.9258648880616172, 0.8919837349601342, 0.9582986126354006, 0.5367788738946208, 0.6523261020662174, 0.8610708629150888, 0.8362644206253728, 0.9914416184415205, 0.5505261545871127, 0.3033441667517738, 0.36303306701676297, 0.39272523769997725, 0.9622598399405435, 0.35448987334562865, 0.970498406225639, 0.9317791102713491, 0.9800309408132742, 0.9616977460883793, 0.9680760824496093, 0.9988123511131078, 0.8868517594474317, 0.6769125417508886, 0.7556484296269644, 0.9840305683976548, 0.9783004748594853, 0.8045320421506815, 0.8584258477370029, 0.7684128947956383, 0.6958888242666597, 0.9762889843100216, 0.9566546235211268, 0.850545080891678, 0.5483459670619044, 0.5925072893586332, 0.6673063867410942, 0.7950336718764637, 0.9725905571653328, 0.7903406643231715, 0.5523671057427535, 0.958329133023918, 0.9871470306944015, 0.9941754424441114, 0.33551151510936894, 0.8896505516373311, 0.9871604752936056, 0.9593901222150246, 0.7863873483788776, 0.33549373405640703, 0.8665999993953842, 0.5005469028937609, 0.9977870202899837, 0.7458982309937792, 0.7922340070646271, 0.9642155839631797, 0.8929632472822456, 0.5308820244504178, 0.38179617584576653, 0.9938649240326018, 0.9654601447951895, 0.982301728586674, 0.6633957602519991, 0.2843806503600278, 0.33288321947952204, 0.9809626697619825, 0.865879435917396, 0.9495749899440564, 0.7496754758483222, 0.4380026765468674, 0.36402209855920165, 0.8635289026902501, 0.8017833787108356, 0.6587506892682935, 0.7424348936636066, 0.35560436474014046, 0.4543003552045187, 0.4119509357486908, 0.9823904255329727, 0.5524385949937208, 0.7471100001593443, 0.9498640418315879, 0.33290532876446466, 0.7048278056763488, 0.7841887621690384, 0.9629941843087008, 0.5027735200361356, 0.7318796500640772, 0.9929423515179944, 0.5772316657124125, 0.7478527262372174, 0.8291896918354393, 0.992216230569648, 0.2685018884905097, 0.6968437131873063, 0.9531522786757555, 0.9714128133545932, 0.9120033145953498, 0.8687624584956963, 0.9783263968393229, 0.7761276945108742, 0.933649774401709, 0.46003690491934934, 0.9368997952075159, 0.9987576103738288, 0.6500235577561153, 0.45306905127145913, 0.9632498758764436, 0.7357282560150715, 0.7185847236665515, 0.4063929339107524, 0.7483433290627493, 0.7782708496295451, 0.37829768791325813, 0.937541658902099, 0.7673625408665599, 0.9951730927376471, 0.9989566047390491, 0.5439260970925807, 0.9836092275351526, 0.7246245693623756, 0.9972221890186312, 0.8148992832673052, 0.6210741702934459, 0.8699744937269255, 0.530113854122283, 0.4524225763158376, 0.9219773467558939, 0.6440778451573138, 0.6456799887346754, 0.9968751603109034, 0.3665086230574258, 0.6615664873363982, 0.8427311198375503, 0.7977175153993858, 0.4400319307982853, 0.8049280320313652, 0.8442322953707048, 0.9890142021204371, 0.2175988627893617, 0.9608951908650563, 0.9632845059387102, 0.9256859914122456, 0.7665677413188677, 0.9957224950300203, 0.7292502332583561, 0.6795554171143636, 0.8689601643155015, 0.6903495623461344, 0.3711670262238431, 0.9971956358155536, 0.5030460529669166, 0.6922845412111027, 0.7985394740453782, 0.86503749919219, 0.9064712554290416, 0.9526027094876495, 0.6300907569775986, 0.9267525347330329, 0.8992975755331606, 0.963733812696403, 0.35944257979349437, 0.35549468312391663, 0.9978859356867157, 0.9642330714538202, 0.9526363444326679, 0.9798867990788087, 0.7345509737075809, 0.6954626004262314, 0.33472594094925745, 0.2317424549313577, 0.9890703604587029, 0.9431909968773261, 0.8398770683322134, 0.2535546949671298, 0.9716364434465974, 0.30631015323753386, 0.7679960958755118, 0.7322133086617189, 0.4349863621100094, 0.49528680088821897, 0.8220767596838926, 0.8193038242164443, 0.9523820113180522, 0.9551332153413034, 0.9775897024919651, 0.9908068521134594, 0.996526522582476, 0.9684003873044024, 0.6793097196305117, 0.8618480053465095, 0.8582457972543853, 0.9951647823214569, 0.8811817450760608, 0.4716723150205292, 0.6862573087019166, 0.9392339983866842, 0.9844157405836365, 0.6340647554918681, 0.6834932561316325, 0.5980151619847569, 0.3977145575538179, 0.792519297929578, 0.8341167573245646, 0.6639034789282829, 0.3157181187254152, 0.33493471938259045, 0.6969531244640554, 0.4847421218899074, 0.8617317125420009, 0.47756458498773435, 0.9400894494487385, 0.3824800723129951, 0.6849177346432338, 0.9583975736900853, 0.9977270766593914, 0.4287886490808994, 0.7550957782907264, 0.3200963143185538, 0.9813434422157983, 0.5507854523859419, 0.9229554880057125, 0.9789528922660828, 0.5853736160253578, 0.7264323733288429, 0.8650311616531561, 0.8591591781457484, 0.9814281238481696, 0.9288705494695217, 0.2897342434851098, 0.2567488869176723, 0.8919074385224578, 0.5114838447077827, 0.32148253595538623, 0.9773784453025673, 0.8496377659501452, 0.6319263983553736, 0.5674531227950543, 0.7750798083019365, 0.6625453565402473, 0.7275391713767485, 0.7509391468031875, 0.5418216062846648, 0.7232188615401234, 0.6057943087281479, 0.761189667067421, 0.6349298690591205, 0.6721336719454992, 0.8575162045306909, 0.696225395236243, 0.9666799037286098, 0.8167962791602748, 0.7450725289332507, 0.9989303696558136, 0.7646866411533457, 0.6761108366204224, 0.6607056279843826, 0.9906076254189732, 0.9945095169230377, 0.9848958733102056, 0.6151107246618468, 0.3885746926678175, 0.524368139358246, 0.44827884899565706, 0.9891716432195719, 0.5259061273286485, 0.8210852976147046, 0.8512037603922222, 0.8095790746014928, 0.40497063478398754, 0.5832228396678061, 0.4710409726748854, 0.998214262599088, 0.7448362901242238, 0.7536518361743272, 0.9814522210596435, 0.9978808254765973, 0.3424076056351296, 0.9702425538824443, 0.9978089169506918, 0.9838741839288063, 0.4063043319348083, 0.9671818781207955, 0.8359532015941746, 0.8123534875973488, 0.4477495595223109, 0.3761069751684747, 0.9715592479930658, 0.8386543025016231, 0.9307243367065033, 0.33869450973677184, 0.9513236094629393, 0.7925334805980625, 0.8401581136886298, 0.35127981999389424, 0.9971833114922948, 0.889514670122606, 0.7683543174684982, 0.82348955503798, 0.985596701455453, 0.35103896884692243, 0.9974428411178332, 0.5055486218625599, 0.8978735536616917, 0.9628561176138483, 0.8018427360250543, 0.5298124814941831, 0.9986882550658142, 0.9973795870874543, 0.9747902538332275, 0.42520577460924047, 0.7690322025748972, 0.9985933502405479, 0.8519210205854181, 0.9450651490160998, 0.34383646620418584, 0.8537713872186772, 0.9928223929997331, 0.9361417023023566, 0.824759296443967, 0.8668889391259188, 0.6918108258319026, 0.9862294514440694, 0.6326743515777595, 0.9315412809497472, 0.9720773049766027, 0.3946245091065329, 0.41223798574989623, 0.8520877826955332, 0.935289163567868, 0.45193323059931756, 0.8129499382139552, 0.9962973208401461, 0.8404467761683652, 0.3106783437791798, 0.5207425292775497, 0.9835134941935668, 0.7577691345091566, 0.9743647995087071, 0.9769589346590278, 0.8939181910889236, 0.6140964557447974, 0.44080599747874816, 0.5920894043191955, 0.8226516516498262, 0.9108621098227122, 0.5112150647970887, 0.40548960870197637, 0.8789917198280506, 0.9986948978038229, 0.9623091591958692, 0.9134124038511569, 0.9856520613841129, 0.3643498493041939, 0.9972265654193372, 0.916278388257858, 0.2933652181363343, 0.796258858560848, 0.6928598203175784, 0.4723662771138748, 0.2996631405151031, 0.6499398319353294, 0.8469015115776533, 0.808801999407686, 0.23666013286738063, 0.7276162241075875, 0.29453403125533095, 0.986001244434375, 0.35303895355134984, 0.9876794712722549, 0.9053131332382189, 0.9359515582429582, 0.48031108068341166, 0.6817462846046162, 0.984393027202134, 0.9772200211936669, 0.7691631871627541, 0.5789548215362676, 0.1948689997153079, 0.9804880104186172, 0.9460445356845129, 0.27707615763796545, 0.9906953614845765, 0.6823949839329184, 0.8952864401964072, 0.8830380445176425, 0.2981153523276344, 0.40841200492838203, 0.7853807200148941, 0.5505042825043462, 0.3835412749549092, 0.9363090615938521, 0.7744968191192786, 0.9705718184627907, 0.9816740794519078, 0.9051109816014079, 0.6710952121968283, 0.7916071239462761, 0.3642777527504253, 0.9142563426229265, 0.4704457415063448, 0.9206808277139491, 0.997254444385585, 0.841857746176042, 0.865649273207912, 0.803911929232067, 0.9947627976513073, 0.9961434419630586, 0.9101170054718153, 0.21583983247629657, 0.7467221677670386, 0.6283822510204216, 0.9976762685549492, 0.9707905052592452, 0.9828556310558174, 0.8040660805250353, 0.9597208596034482, 0.27165039867150953, 0.5700910637568996, 0.9366623134315093, 0.49737817871933665, 0.9568660333008084, 0.5489200376666572, 0.790296684283236, 0.673047706164775, 0.7620490702169944, 0.9303510700063939, 0.7264976797521616, 0.5220511758320754, 0.9762913015067547, 0.33297460780790467, 0.48768570723878124, 0.9506939622364265, 0.9037944345808103, 0.26277198142411495, 0.9393169338911456, 0.9980878199646099, 0.2527437017076676, 0.9971052287845965, 0.9017957315880724, 0.401020060510882, 0.9799063052301705, 0.6876773555685829, 0.6106406126486918, 0.7297268398865536, 0.9348523032262275, 0.9444348141800789, 0.7570454625977762, 0.9974435752557496, 0.660736949168003, 0.9309628930506672, 0.9520325470755214, 0.9971111796505545, 0.8472122444231006, 0.9711204531671356, 0.9971783219322768, 0.9179669017655223, 0.5807666081352869, 0.9806386513026293, 0.5959458159141873, 0.27788831471561015, 0.5597739344625847, 0.9009288483748359, 0.9966429240004919, 0.9890708001684054, 0.6228836625259282, 0.37390588810011927, 0.6709779911911028, 0.9922998554289189, 0.39928846553770203, 0.2495371510487514, 0.9736341829724976, 0.9582283103504496, 0.8798905112027717, 0.9050990358229866, 0.9071913940695296, 0.3241346394004707, 0.9340212926176823, 0.7879258675862895, 0.3692567365695394, 0.809283663064442, 0.52178001743253, 0.9822172398515899, 0.8781626583903668, 0.8924162428316382, 0.7929999604875988, 0.9131644123827697, 0.9385279195767069, 0.7448362901242238, 0.9485693397224372, 0.4995990670962861, 0.7715500823435787, 0.2622930270577357, 0.8241545243163555, 0.8921460768780356, 0.9627415417997576, 0.7829470105574494, 0.2981116586276118, 0.5722304846548192, 0.38438324543188307, 0.3606357021205384, 0.34021838857246045, 0.6382665168474604, 0.8906493420595479, 0.7696404246720312, 0.676771695234297, 0.47938733447307613, 0.28143785669909316, 0.41179579320829796, 0.9831001978776241, 0.622776350037915, 0.8946477939139982, 0.4407682594426269, 0.9392567750853571, 0.401372615679471, 0.7476445123940136, 0.23026066884869234, 0.9076741685678372, 0.7060712798712916, 0.9590310177281574, 0.5497348623452019, 0.8542577414416384, 0.953219029932413, 0.419972521413979, 0.29791155932263946, 0.8673153086176418, 0.8103258026481821, 0.47732425168671777, 0.824969118081754, 0.978497301984604, 0.9873732897310292, 0.8889786601335371, 0.6389677178348728, 0.643689638279646, 0.8703973382704046, 0.5019914830498298, 0.7922156478071144, 0.30133270435209614, 0.3213851243289201, 0.45276244963489976, 0.7599113469867175, 0.9198265512430104, 0.6556427609357076, 0.8101317496715619, 0.8367814826406073, 0.9537095277485274, 0.9761033015319344, 0.5092346911027498, 0.8857755073005298, 0.1767567174639627, 0.4410942905774078, 0.5025678950858763, 0.7734372657099868, 0.9029896371046469, 0.8982940006207353, 0.6218324294652147, 0.5230200099180329, 0.5801780368886587, 0.9874897781934967, 0.7028534014440133, 0.9499342280503902, 0.7981627994789529, 0.9389820288809602, 0.9002065397096771, 0.9774702196104779, 0.2737586355400833, 0.7451713057185987, 0.7264956053546706, 0.7666793475969726, 0.9339366344726432, 0.9384157267807564, 0.8470830247179187, 0.5234658252676521, 0.85661471453889, 0.93388423656751, 0.940573679608233, 0.3243807212012194, 0.9912165038034106, 0.4242594216265687, 0.8702500749958199, 0.9947632040845806, 0.9864111434149797, 0.9958138355911661, 0.22185641758369923, 0.4899727886410994, 0.372925701297523, 0.6793126516474768, 0.8703272647018456, 0.3141531508878048, 0.533921150977694, 0.7529017529225298, 0.9848275857479745, 0.7093490689097635, 0.3743377427158051, 0.5774022890356749, 0.5836123525926797, 0.7272075127026507, 0.9902928672821263, 0.5628452213655196, 0.9176989022156035, 0.961933477176291, 0.9784573029027949, 0.9832146346798346, 0.41391646702966567, 0.9363502013343495, 0.8180358071347018, 0.28868507423890044, 0.47075279708923046, 0.9302314166120307, 0.5932121452581463, 0.7641689175513492, 0.9153487044156139, 0.3708198667285031, 0.9088226427943038, 0.23968126867821504, 0.8349144078893613, 0.9752306424212355, 0.996729057892241, 0.9392146694287716, 0.8301308563654916, 0.2665930151817557, 0.8542376362766972, 0.8203790810832104, 0.984425157085769, 0.7639966117080896, 0.3643117225944381, 0.9946323793863715, 0.9923166566535491, 0.6478414585307386, 0.3029297826343582, 0.9988741538781118, 0.40427431666767055, 0.8989925941619039, 0.8744760009056731, 0.515638537824999, 0.5946660951671952, 0.408362010306933, 0.7598744600930518, 0.917367908334805, 0.9195845399733387, 0.988295798063279, 0.9973051581218544, 0.46401841773094565, 0.9184729658187981, 0.8346221210828184, 0.456540822710378, 0.9677886546031482, 0.8847508802803862, 0.26121169441702985, 0.7508188039564517, 0.754171079239791, 0.9641749567500665, 0.9077972370000855, 0.5841033115258869, 0.8681285250208156, 0.9067144252888294, 0.5139297925165878, 0.8797804678122525, 0.39465832848989013, 0.3116658573306288, 0.9559232184368757, 0.4518034086281397, 0.9984496310707076, 0.6077607867872665, 0.9970074039222709, 0.8731065955720752, 0.8075030608221411, 0.9905356797039255, 0.9808211151515872, 0.9977595115708058, 0.3016432878972286, 0.9662164046607341, 0.6226052479303681, 0.7869423164026024, 0.7478961538694693, 0.8365647937857678, 0.9474204983381841, 0.9978043052147821, 0.7852638584955189, 0.9981679582265132, 0.8892442853031531, 0.9472321711926339, 0.6250010476946417, 0.7755395289270182, 0.6056019062196765, 0.7545444473370286, 0.6812790427729098, 0.3689423637725011, 0.9415788723239629, 0.9736142398847523, 0.7362445955501706, 0.7797800587247261, 0.732106135231831, 0.976470883601135, 0.7727979445197959, 0.9867517391508687, 0.8442362818248598, 0.9941612075705568, 0.5334178032959018, 0.9038013599826804, 0.6409725763600537, 0.8349812362584813, 0.7306368636180389, 0.8987420689544949, 0.9354962331978257, 0.9690863596309118, 0.9570582984439665, 0.8925506307252233, 0.9984506901547634, 0.9617818328909392, 0.9717699280426623, 0.8729611334930079, 0.44976509812978427, 0.4303711517611287, 0.8538077118150988, 0.9985681699757594, 0.7196519158842288, 0.6442612980006976, 0.8957983336294698, 0.4566964740974264, 0.9800001121490666, 0.9667182497912957, 0.5339854583785161, 0.9748188393847175, 0.926719898404288, 0.5196947390188897, 0.5982131641413764, 0.927523988591481, 0.924856220946538, 0.36606875938573635, 0.9621330205033826, 0.3914075875188703, 0.845633108819152, 0.3882850145401924, 0.615340652436778, 0.7915310555885882, 0.3585393713269627, 0.957788316717612, 0.7651697993657638, 0.9982934283309068, 0.9982696974776049, 0.942333633396885, 0.9834168978710438, 0.7702747501653489, 0.2805866791940109, 0.9730218624721845, 0.3291446756258416, 0.4023678674887742, 0.7453734618163563, 0.5916063723129628, 0.992474919780252, 0.719260387089445, 0.9181811393949255, 0.9877043592066251, 0.7381627459077236, 0.8790033136703005, 0.5404453780251731, 0.8837473759281852, 0.9789979091823502, 0.48424963864494397, 0.601104901052111, 0.8183593791025116, 0.9917348100320363, 0.42424940231589825, 0.9964797828145564, 0.47789025792081796, 0.9664234900208458, 0.3926641261181737, 0.9515218132606731, 0.4842129708212739, 0.9858273924400137, 0.8907360688829016, 0.9974408496491963, 0.6371459657594548, 0.48794245101596956, 0.6996518991685927, 0.9922057363967186, 0.3737618037436299, 0.8496174285459359, 0.8866979706068141, 0.7893336892851973, 0.9663078990850189, 0.9876747326629451, 0.9833145679323148, 0.5440024605571177, 0.9462232004844984, 0.47599324476674765, 0.9959565122454016, 0.9325907012088889, 0.5441023541693186, 0.9465989329542268, 0.9743798327840394, 0.7928928516952423, 0.7443633809250054, 0.6465826193336217, 0.9827788412779987, 0.8725513912740555, 0.46046135012074285, 0.8606626826781829, 0.6465947512644228, 0.7850729543639665, 0.7561529535652427, 0.19730763529437534, 0.697802816940919, 0.8379658792995103, 0.9647826569618922, 0.993283900601157, 0.727080747919464, 0.9444598543851593, 0.4252166400704557, 0.9974820863388442, 0.9666920774853462, 0.9974784074379056, 0.9112848625925744, 0.9707355375920085, 0.917416445238939, 0.8698086758363233, 0.9560806039187073, 0.880422847158749, 0.3349135235143146, 0.9779184976059577, 0.9503784395261652, 0.30798134637159713, 0.5820687382214336, 0.9569953880895308, 0.798494089369134, 0.5688554124699376, 0.7747232894108482, 0.44910593189185655, 0.8756672567302378, 0.4538228331964774, 0.9692580858683004, 0.8358199470862284, 0.9914907158552758, 0.32614087402338254, 0.47979220051601246, 0.916905759224412, 0.9928823867188322, 0.8897622419521454, 0.9370813628810557, 0.5922418939839313, 0.8988012981582637, 0.7616085911885898, 0.7834778928439831, 0.746585140511614, 0.5782762470881883, 0.9786358452959362, 0.9702671059397258, 0.7101175217471195, 0.49547149861722917, 0.984767042250551, 0.85635866180559, 0.9553538751564452, 0.615526162191452, 0.3765652240684523, 0.7089659158503955, 0.9045609045062336, 0.39989015340570155, 0.7572581347767755, 0.9980081713005521, 0.9404748267475188, 0.9483166885670906, 0.9750894603418535, 0.9062456712658018, 0.6623163656507958, 0.7525511074122225, 0.28960971510127254, 0.6631726945453518, 0.4335679833330872, 0.8929135131382804, 0.7003603228427264, 0.8926520427774359, 0.2607145076986904, 0.9978102378414467, 0.3904209618015051, 0.7725657675999154, 0.6590566402519291, 0.9919970676444833, 0.41099048565640206, 0.9901955594403797, 0.303601054012992, 0.9720515617374657, 0.6438303651269919, 0.4510790490478509, 0.6779135666407073, 0.9962519033628183, 0.801399753173756, 0.5520104358120315, 0.2717523306704729, 0.5746721155112158, 0.9935962615003275, 0.29382006397103533, 0.46560787002264914, 0.5316833722751995, 0.9753841278822399, 0.994541326644076, 0.7165593135514485, 0.8837544631316738, 0.9560042779562548, 0.6821012700493456, 0.5871366112429042, 0.9762867775442442, 0.9433467876411432, 0.4400175424651537, 0.9939758352657051, 0.9545958845564947, 0.5643789349964813, 0.7860393235375127, 0.7438750900603879, 0.9949850665237143, 0.8783101113152192, 0.21069655153143085, 0.3049744280936295, 0.9988770084277915, 0.9365567607286006, 0.9623909551470923, 0.9884382230449819, 0.9899976187461441, 0.30064638331239113, 0.8290968350384512, 0.3884110330787245, 0.3135562010666476, 0.7242348070996165, 0.9913235378451838, 0.4077853479253746, 0.944449534630282, 0.9482822550874692, 0.6885863436485813, 0.9290830677509857, 0.9643422380256264, 0.9909347954395861, 0.6949460511918566, 0.6092430509358316, 0.576919438338545, 0.5769699816455856, 0.6460028980314962, 0.8768950585017413, 0.9901334181414283, 0.9656080275197141, 0.8997799162428864, 0.6887691297993688, 0.9935451631301443, 0.6808298564142239, 0.9891714131955589, 0.7509322919320175, 0.9557684601894565, 0.7255005382506392, 0.9912087081934474, 0.9958321333338713, 0.5053453699050667, 0.9959650424616411, 0.3987428427724462, 0.42317822474508293, 0.9769508255048817, 0.2956849434908537, 0.9406541388326358, 0.8181485612099184, 0.8548615308099383, 0.2878234515920176, 0.6398459504083702, 0.7582716788452024, 0.7175577604705172, 0.44695882584630686, 0.9738095475465222, 0.9957829558741558, 0.9002654960801679, 0.8222931680192733, 0.21396117926998548, 0.9234162326212684, 0.9881959039374432, 0.24051394307250362, 0.8005174759997533, 0.9818862714872503, 0.7901878846286827, 0.8652060699251451, 0.9967471265655234, 0.7830051477738376, 0.9749091518986238, 0.4465781164689827, 0.8232596588247444, 0.6993905516207459, 0.9968951136998988, 0.9004453341943238, 0.9974131678773699, 0.9976530547466199, 0.7613299228361968, 0.7890787974911256, 0.7859361834808679, 0.8567438602863923, 0.4978530944182391, 0.9049288650277539, 0.9642662698633491, 0.74035586578835, 0.6345787271987791, 0.5704344675314295, 0.9977698159889932, 0.48545580948098155, 0.754186652156416, 0.7302306351787027, 0.8930613669869576, 0.9959174077329237, 0.988155126420498, 0.43702578191477404, 0.9699405140358434, 0.9723349285065338, 0.30766783804270204, 0.8975940975307881, 0.9944486613697356, 0.8679934651158259, 0.6782070206519597, 0.9779471345186773, 0.5570149109938327, 0.8620422077181171, 0.9906659240095791, 0.41299068014403595, 0.8626105852196619, 0.9640274248371115, 0.8169364139314519, 0.9224074037613826, 0.8014649285583674, 0.28632576266864695, 0.7076274819399413, 0.7503634050015018, 0.9852605174936708, 0.4269903052755333, 0.5581703457751749, 0.5636790692560414, 0.3533222017072535, 0.8743309546110565, 0.9987603546151256, 0.424565807275193, 0.7685916402742099, 0.8200997596590619, 0.9048006507573509, 0.8006870218121117, 0.965231525490169, 0.7701914402162211, 0.24866094889157597, 0.976940411772233, 0.2930552361506098, 0.9940537160575055, 0.5459807151326448, 0.34279208483475454, 0.9187099993419817, 0.6356502652635707, 0.9871321182795303, 0.9385251931897428, 0.3242233659069792, 0.800064112877387, 0.9820914461383174, 0.34778339420819326, 0.9853625844765358, 0.3688940757907464, 0.6672071310912131, 0.9832857732441753, 0.7375704234988542, 0.3455450408798164, 0.8160237074191883, 0.9967364987315945, 0.9975270530648488, 0.5897588740753987, 0.4088007801764184, 0.4142371077738053, 0.9029920436132998, 0.6301041590592846, 0.9627018562131011, 0.29801859496882177, 0.4092462555114019, 0.4139805546883981, 0.8640534842026176, 0.4996869714171047, 0.7676721234226179, 0.4719745244069573, 0.9940975811848043, 0.9958752952716425, 0.9979076954543443, 0.4134768480606359, 0.26764495489794693, 0.5921522157856813, 0.9796916533776658, 0.9792981070006153, 0.37564910947568564, 0.705152019683142, 0.8329300811167641, 0.7633128997615657, 0.8611467070970028, 0.9896939843550658, 0.34019520102165535, 0.6378215292889249, 0.5302317192856432, 0.7631127513830078, 0.838071794225177, 0.8631646714218438, 0.44809304589762833, 0.6204061958370359, 0.9307468461245633, 0.9472552617403117, 0.4500422139838347, 0.603341237317681, 0.3093256546980998, 0.937041423714837, 0.6349022783659755, 0.26330474827494466, 0.9912207106146607, 0.9968745871857884, 0.7609054366103387, 0.32961985996625, 0.885543362709752, 0.9894912184683226, 0.32195454488967756, 0.9268235258479688, 0.4295724857980301, 0.6050734976552654, 0.9800010700010472, 0.836979556119256, 0.9977737067823559, 0.8543339968156175, 0.5941180200479483, 0.9978365234137796, 0.6665057521341071, 0.9855444321535807, 0.9817329488154183, 0.17789398709477444, 0.9675441900057806, 0.9618326263279663, 0.9793343603366117, 0.47500722279497765, 0.625321995573942, 0.32855831779291944, 0.7000802026370987, 0.17384276214615402, 0.31219342481132173, 0.45023982580791855, 0.3277838613548857, 0.3061488612587868, 0.9632051899995894, 0.33206961838431903, 0.31425532957640934, 0.5028869076197939, 0.4270942405170616, 0.9253868421007108, 0.8566829334812984, 0.2875101014036209, 0.7446533109750411, 0.9621077332905981, 0.8796877362073919, 0.9987298102682433, 0.954998488557315, 0.4375370104494729, 0.43353390931777114, 0.8018413272821046, 0.4396290212109021, 0.6407440350664217, 0.7248027861140405, 0.9940585321111689, 0.9762300525174961, 0.3551108868416619, 0.5007592863426638, 0.7205135645075749, 0.9487017837103708, 0.9908353467149865, 0.9496729570463205]\n",
      "[1 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eduar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to coerce to Series, length must be 19: given 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-f97d1df517e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mregressao_logistica\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mregressao_logistica\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"red\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\arraylike.py\u001b[0m in \u001b[0;36m__add__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__add__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__radd__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   5978\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m  \u001b[1;31m# only relevant for Series other case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5980\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malign_method_FRAME\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5982\u001b[0m         \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch_frame_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36malign_method_FRAME\u001b[1;34m(left, right, axis, flex, level)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m             \u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mto_series\u001b[1;34m(right)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    220\u001b[0m                     \u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgiven_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m                 )\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to coerce to Series, length must be 19: given 1"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.special import expit\n",
    "\n",
    "#Como esse algoritmo só funciona com float, vamos ter que fazer labels numericas.\n",
    "\n",
    "\n",
    "X = df.drop(['Churn','customerID'], axis=1)\n",
    "y = df['Churn']\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Vamos trocar labels em strings por labels numéricas no train seguindo:\n",
    "#Passo 1 - Ident. colunas categoricas\n",
    "#Passo 2- Aplicar o label encoder somente nas categoricas\n",
    "#Guardar esse objeto para usar no teste\n",
    "\n",
    "colunas_categoricas =[]\n",
    "\n",
    "for column in X_train:\n",
    "    try:\n",
    "        X_train[column].iloc[0].isnumeric() == False\n",
    "\n",
    "    except AttributeError:\n",
    "        print (type(X_train[column][0]))\n",
    "\n",
    "    else:\n",
    "        colunas_categoricas.append(column)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for coluna in colunas_categoricas:\n",
    "    le.fit(X_train[coluna])\n",
    "    X_train[coluna] = le.transform(X_train[coluna])\n",
    "    X_test[coluna] = le.transform(X_test[coluna])\n",
    "\n",
    "\n",
    "\n",
    "#Vamos fazer nossa árvore de decisão setando todos os parâmetros como default\n",
    "\n",
    "\n",
    "regressao_logistica = LogisticRegression(random_state=0,\n",
    "                                      penalty=\"l2\",\n",
    "                                      dual=False,\n",
    "                                      tol = 1e-4,\n",
    "                                      fit_intercept = True,\n",
    "                                      solver = 'lbfgs',\n",
    "                                      max_iter=100)\n",
    "\n",
    "regressao_logistica.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = regressao_logistica.predict(X_test)\n",
    "y_probabilities_old = regressao_logistica.predict_proba(X_test)\n",
    "\n",
    "y_probabilities = []\n",
    "\n",
    "for i in range(0,len(y_probabilities_old)):\n",
    "    y_probabilities.append(y_probabilities_old[i][0])\n",
    "\n",
    "f1 = f1_score(y_test,y_predicted)\n",
    "precisao = accuracy_score(y_test,y_predicted)\n",
    "print(\"O método de arvore de decisão teve um score de treino de: \", precisao, \"% de acurácia e um F1-Score de:\", f1)\n",
    "\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.scatter(y_probabilities, y_predicted, color=\"black\", zorder=20)\n",
    "\n",
    "\n",
    "loss = expit(X_test * regressao_logistica.coef_ + regressao_logistica.intercept_).ravel()\n",
    "plt.plot(X_test, loss, color=\"red\", linewidth=3)\n",
    "\n",
    "\n",
    "\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.xticks(range(-5, 10))\n",
    "plt.yticks([0, 0.5, 1])\n",
    "plt.ylim(-0.25, 1.25)\n",
    "plt.xlim(-4, 10)\n",
    "plt.legend(\n",
    "    (\"Logistic Regression Model\"),\n",
    "    loc=\"lower right\",\n",
    "    fontsize=\"small\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# fig = plt.figure(figsize=(25,20))\n",
    "# _ = tree.plot_tree(arvore_de_decisao,   \n",
    "#                   class_names='Churn',\n",
    "#                    filled=True)\n",
    "\n",
    "# fpr, tpr, _ = roc_curve(y_test, y_predicted)\n",
    "\n",
    "# plt.clf()\n",
    "# plt.plot(fpr, tpr)\n",
    "# plt.xlabel('FPR')\n",
    "# plt.ylabel('TPR')\n",
    "# plt.title('ROC curve')\n",
    "# plt.show()\n",
    "\n",
    "# path = avore_de_decisao.cost_complexity_pruning_path(X_train,y_train)\n",
    "# alphas=path['ccp_alphas']\n",
    "\n",
    "# alphas\n",
    "\n",
    "# confusion_matrix = pd.crosstab(y_test, y_predicted, rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "# sns.heatmap(confusion_matrix, annot=True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusões Da Análise\n",
    "\n",
    "\n",
    "A maior lição que podemos tirar destes dados é que pessoas que contratam os serviços extra da empresa são menos propicias a sair do que pessoas que só contratam o serviço de internet.\n",
    "\n",
    "também foi possível encontrar uma dispariedade de Churn entre pessoas que pagam com Electronic check e Mailed Check. Parece que pessoas que pagam com Electronic Check sairam mais dos serviços do que aqueles que pagam com Mailed Check.\n",
    "\n",
    "\n",
    "Uma possível solução para mitigar o problema de churn da empresa é promover os serviços adicionais junto com o pacote de internet. Deste modo os clientes vão ter conhecimento destes serviços e podem contrata-los. A empresa também pode oferecer estes serviços de graça para pessoas que desejam cancelar a assinatura, garantindo-os como clientes.\n",
    "\n",
    "\n",
    "Não podemos ignorar que a grande parte dos churn são de assinantes de fibra ótica. Será que temos algum problema com o serviço? Vale a pena investigar."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e6cd1a37ae743b95addcaf84b2b82dd57fbb64481c9e7c7a479f657aeb68d7c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
